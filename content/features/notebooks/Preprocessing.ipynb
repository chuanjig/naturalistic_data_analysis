{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Preprocessing\n",
    "*Written by Luke Chang*\n",
    "\n",
    "There are currently no agreed upon conventions for preprocessing naturalistic neuroimaging data. In this tutorial, we show how we preprocessed the datasets used in all of our tutorials. This tutorial assumes you have some basic knowledge of preprocessing. If you have questions about the specific details, we enourage you to read other tutorials, such as the preprocessing [overview](https://dartbrains.org/features/notebooks/7_Preprocessing.html) from the Dartbrains course. Various contributors also share their personal thoughts on issues they consider in their own labs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naturalistic Data Preprocessing\n",
    "For the two datasets we are using in this course (Sherlock & Paranoia), we performed very minimal preprocessing. First, we used [fmriprep](https://fmriprep.readthedocs.io/en/stable/) to realign and spatially normalize the data. If you don't have strong opinions about the details of preprocessing, we highly recommend using fmriprep, which is developed and maintained by a team at the [Center for Reproducible Research](http://reproducibility.stanford.edu/) led by Russ Poldrack and Chris Gorgolewski. Fmriprep was designed to provide an easily accessible, state-of-the-art interface that is robust to variations in scan acquisition protocols, requires minimal user input, and provides easily interpretable and comprehensive error and output reporting. We like that they share a docker container with all of the relevant software packages, it is very simple to run, and that there is a large user base that actively report bugs so that it is constantly improving.\n",
    "\n",
    "After preprocessing with fmriprep, we smoothed the data (fwhm=6mm) and performed basic voxelwise denoising using a GLM. This entails including the 6 realignment parameters, their squares, their derivatives, and squared derivatives. We also include dummy codes for spikes identified from global signal outliers and outliers identified from frame differencing (i.e., temporal derivative). We chose to not perform high-pass filtering and instead include linear & quadratic trends, and average CSF activity to remove additional physiological and scanner artifacts. Finally, to save space, we downsampled to Float32 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltools.stats import regress, zscore\n",
    "from nltools.data import Brain_Data, Design_Matrix\n",
    "from nltools.stats import find_spikes \n",
    "from nltools.mask import expand_mask\n",
    "\n",
    "def make_motion_covariates(mc, tr):\n",
    "    z_mc = zscore(mc)\n",
    "    all_mc = pd.concat([z_mc, z_mc**2, z_mc.diff(), z_mc.diff()**2], axis=1)\n",
    "    all_mc.fillna(value=0, inplace=True)\n",
    "    return Design_Matrix(all_mc, sampling_freq=1/tr)\n",
    "\n",
    "base_dir = '/Volumes/Engram/Data/Sherlock/fmriprep'\n",
    "\n",
    "fwhm=6\n",
    "tr = 1.5\n",
    "outlier_cutoff = 3\n",
    "\n",
    "file_list = [x for x in glob.glob(os.path.join(base_dir, '*/func/*preproc*gz')) if 'denoised' not in x] \n",
    "for f in file_list:\n",
    "    sub = os.path.basename(f).split('_')[0]\n",
    "\n",
    "    data = Brain_Data(f)\n",
    "    smoothed = data.smooth(fwhm=fwhm)\n",
    "\n",
    "    spikes = smoothed.find_spikes(global_spike_cutoff=outlier_cutoff, diff_spike_cutoff=outlier_cutoff)\n",
    "    covariates = pd.read_csv(glob.glob(os.path.join(base_dir, sub, 'func', '*tsv'))[0], sep='\\t')\n",
    "    mc = covariates[['trans_x','trans_y','trans_z','rot_x', 'rot_y', 'rot_z']]\n",
    "    mc_cov = make_motion_covariates(mc, tr)\n",
    "    csf = covariates['csf'] # Use CSF from fmriprep output\n",
    "    dm = Design_Matrix(pd.concat([csf, mc_cov, spikes.drop(labels='TR', axis=1)], axis=1), sampling_freq=1/tr)\n",
    "    dm = dm.add_poly(order=2, include_lower=True) # Add Intercept, Linear and Quadratic Trends\n",
    "\n",
    "    smoothed.X = dm\n",
    "    stats = smoothed.regress()\n",
    "    stats['residual'].data = np.float32(stats['residual'].data) # cast as float32 to reduce storage space\n",
    "    stats['residual'].write(os.path.join(base_dir, sub, 'func', f'{sub}_denoise_smooth{fwhm}mm_task-sherlockPart1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributors' Opinions on Preprocessing\n",
    "\n",
    "## Luke Chang\n",
    "In our work, we perform standard realignment and spatial normalization. We also perform basic denoising by removing global and multivariate spikes, average CSF activity, linear/quadratic trends, and 24 motion covariates (e.g., 6 centered realignment, their squares, derivative, and squared derivatives). We are very cautious about performing high-pass filtering as many of the effects we are interested in occur in slower frequencies. We find that including average activity from a CSF mask helps a lot in reducing different types of physiological and motion related artifacts. We typically apply spatial smoothing, but depending on the question we don't always perform this step. For spatial feature selection, we rarely use searchlights and instead tend to use parcellations. This allows us to quickly prototype analysis ideas using smaller numbers of parcels (e.g., n=50) and then increase the number if we want greater spatial specificity. We usually use a [parcellation](https://neurovault.org/collections/2099/) that we developed based on meta-analytic coactivation using the neurosynth database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T02:34:11.627082Z",
     "start_time": "2020-05-15T02:34:11.622577Z"
    }
   },
   "source": [
    "Have thoughts on preprocessing?  Please share them as a github [issue](https://github.com/naturalistic-data-analysis/naturalistic_data_analysis/issues) on our jupyter-book repository and we can incorporate them into the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
