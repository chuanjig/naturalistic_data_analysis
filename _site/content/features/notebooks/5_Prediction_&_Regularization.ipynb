{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Analysis\n",
    "\n",
    "*Written by Jin Cheong & Luke Chang*\n",
    "\n",
    "In this lab we are going to learn about more advanced topics in regression analysis.\n",
    "- How to use link functions to perform regressions on other types of data such as categorical variables\n",
    "- How to perform cross-validation to assess degree of overfitting\n",
    "- How to deal with multicollinearity with regularization. \n",
    "\n",
    "After the tutorial you will have the chance to apply the methods on a separate dataset. \n",
    "\n",
    "\n",
    "## Introduction to logistic regression \n",
    "\n",
    "A logistic regression, also called a logit model, is used to model outcome variables that are binary. Examples of such variables are \"hit versus miss,\" \"success versus fail,\" or  \"male versus female.\" We can use a logistic regression to calculate the probability of an item being one or the other.\n",
    "\n",
    "In the logit model, the log odds of the outcome is modeled as a linear combination of the predictor variables. \n",
    "Lets recall that the simple linear regression had the following form :\n",
    "### $y = \\beta_0 + \\beta_1\\cdot x $\n",
    "\n",
    "We can extend this to a multiple linear regression with multiple features into the following form : \n",
    "### $y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$   \n",
    "\n",
    " \n",
    "In logistic regression, instead of estimating the __value__ of Y from Xs, we estimate the __probability__ of Y. For instance if our model tries to predict whether a person is male(Y=0) or female(Y=1), then a Y value of .8 would mean it has 80% chance of being a female.\n",
    "Formally, the linear regression model is passed into a sigmoid function. Here is the simple form with one predictor variable.\n",
    "## $P(Y) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1)}}$\n",
    "\n",
    "We can easily extend this to multiple regression with n-number of predictors.\n",
    "\n",
    "\n",
    "## $P(Y) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}$\n",
    "\n",
    "Now let's see how we can achieve this in code.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:48:11.684432",
     "start_time": "2017-01-27T10:48:11.679536"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression analysis\n",
    "\n",
    "Let's start off by plotting a sigmoid function and add necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:34:03.578429",
     "start_time": "2017-01-27T10:34:03.408176"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a sigmoid function\n",
    "plt.figure(figsize=(5,5))  # open a figure base and determine the size (width, height)\n",
    "\n",
    "def sigmoid(t):                          # Define the sigmoid function\n",
    "    return (1/(1 + np.e**(-t)))    \n",
    "\n",
    "plot_range = np.arange(-6, 6, 0.1)       # range of x values to plot\n",
    "y_values = sigmoid(plot_range)\n",
    "\n",
    "# Plot curve\n",
    "plt.plot(plot_range,   # X-axis range\n",
    "         y_values,          # Predicted values\n",
    "         color=\"red\",linewidth=3)\n",
    "plt.title(\"Example of a sigmoid function\",fontsize=18)\n",
    "plt.ylabel(\"Probability\",fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load our sample data and create a 'Alcabuse' variable from 'Dalc' and 'Walc' (weekday and weekend alcohol consumption level, respectively) and make it a binary variable by calling it alcohol abuse if a student scores more than 8 on drinking alcohol in a week. The dataset was retrieved from [here](http://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION#) but slightly modified to better illustrate our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:34:28.969833",
     "start_time": "2017-01-27T10:34:28.958171"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data \n",
    "df = pd.read_csv('../Data/student-mat.csv')\n",
    "df['Alcabuse']=0\n",
    "df.loc[df['Dalc']+df['Walc'] >=8,'Alcabuse']=1 # Let's call it abuse if more than 8 alcohol consumption per week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis we want to test is : __\"A student is more likely to abuse alcohol if he has more free time.\"__  \n",
    "'Alcabuse' will be our response variable Y, measuring the number of drinks per week.   \n",
    "'freetime' will be our predictor variable X, measuring the degree of free time.\n",
    "\n",
    "We will use the logit function in the statsmodels package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:34:38.993233",
     "start_time": "2017-01-27T10:34:38.969608"
    }
   },
   "outputs": [],
   "source": [
    "logm = smf.logit('Alcabuse ~ freetime',data=df).fit()\n",
    "print(logm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting logistic model coefficients for prediction\n",
    "\n",
    "We ran our first logit model but we cannot interpret the coefficients like we did in the linear regression model.   Because we applied a logit transform, the coefficients are in log-odds. \n",
    "To make them more interpretable we can exponentiate the coefficients and interpret them as either probability or odds-ratios. \n",
    "\n",
    "### Interpreting coefficients as probabilities\n",
    "To change our log-odds coefficients into probabilities we pass the log-odds value of Y into a sigmoid function.\n",
    "For instance, let's see what the probability is for a student to be an alcohol abuser if he had a level 5 of free time (x=5).  \n",
    "\n",
    "$x = 5$   \n",
    "$\\beta_0 = -7.6262$  \n",
    "$\\beta_1 = 1.6416$  \n",
    "## $ Y = \\beta_0 + \\beta_1\\cdot x = -7.6262 + 1.6416 \\cdot 5$ \n",
    "## $P(Y) = \\frac{1}{1 + e^{-(Y)}} = \\frac{1}{1 + e^{-(-7.6262 + 1.6416 \\cdot 5)}}$\n",
    "\n",
    "We implement this with the following code by using the 'sigmoid' function as defined earlier in this module.\n",
    "We also plot the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:35:53.052903",
     "start_time": "2017-01-27T10:35:53.043411"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 5\n",
    "print('x = %i' %x) # x\n",
    "print('b0 = %f' %logm.params[0]) # b0\n",
    "print('b1 = %f' %logm.params[1]) # b1\n",
    "palcabuse = sigmoid(logm.params[0] + logm.params[1] * x)\n",
    "print('palcabuse = %f' %palcabuse)\n",
    "print('If a student has %i hours of free time per week, then he has %.2f %% probability of alcohol abuse' % (x,palcabuse*100))\n",
    "# Note that we had to use double percent sign %% to print % \n",
    "# because a single % is reserved for printing inline numbers as in %i or %.2f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:37:07.982811",
     "start_time": "2017-01-27T10:37:07.767818"
    }
   },
   "outputs": [],
   "source": [
    "# Plot model fit\n",
    "xs =  np.arange(min(df.freetime)-1,max(df.freetime)+1,0.1)\n",
    "ys = np.array([sigmoid(logm.params[0] + logm.params[1] * x) for x in xs])\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(xs,ys,linewidth=3)\n",
    "jitter = np.random.random(len(df.Alcabuse))/5\n",
    "ys_outcomes = np.abs((df.Alcabuse==1)-0.01-jitter)\n",
    "alpha = 0.7\n",
    "plt.plot(df.freetime[df.Alcabuse == 1], ys_outcomes[df.Alcabuse ==1], 'b.', alpha=alpha)\n",
    "plt.plot(df.freetime[df.Alcabuse == 0], ys_outcomes[df.Alcabuse ==0], 'r.', alpha=alpha)\n",
    "plt.xlabel(\"Freetime\",fontsize=16)\n",
    "plt.ylabel(\"Probability of being an alcohol abuser\",fontsize=16)\n",
    "plt.title(\"Predicting alcohol abuse by free time\",fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting coefficients as odds\n",
    "In some cases it could be more useful to consider the odds of an event happening. \n",
    "For example, we might want to be able to make statementes such as \"If a person smokes, he is 10 times more likely to get lung cancer compared to a person who doesn't smoke.\"  The odds of an event happening is defined by\n",
    "\n",
    "## $odds = \\frac{P(\\text{event})}{P(\\text{no event})} = \\frac{P(\\text{Y})}{P(\\text{not Y})}$\n",
    "while $P(\\text{Y}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1)}}$\n",
    "\n",
    "and $P(\\text{not Y}) = 1 - P(\\text{Y})$\n",
    "\n",
    "If we want to look at how much the odds change for a unit increase in free time, we can use the following \n",
    "\n",
    "## $\\triangle odds = \\frac{\\text{odds after a unit change in predictor}}{\\text{original odds}} $\n",
    "\n",
    "So, if we want to calculate the change in odds ratios of being an alcohol abuser when free time increases from 4 to 5, we can calculate the following: \n",
    "## $\\triangle odds = \\frac{\\text{odds(Y) when x = 5}}{\\text{odds(Y) when x = 4}}  = \\frac{\\frac{\\text{P(Y) when x = 5}}{\\text{1 - (P(Y) when x = 5)}}}{\\frac{\\text{P(Y) when x = 4}}{\\text{1 - (P(Y) when x = 4)}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:39:10.389767",
     "start_time": "2017-01-27T10:39:10.383370"
    }
   },
   "outputs": [],
   "source": [
    "odd1 = sigmoid(logm.params[0] + logm.params[1] * 5) / (1-sigmoid(logm.params[0] + logm.params[1] * 5))\n",
    "odd2 = sigmoid(logm.params[0] + logm.params[1] * 4) / (1-sigmoid(logm.params[0] + logm.params[1] * 4))\n",
    "dodds = odd1/odd2\n",
    "print('A student with 5hrs of free time compared to a student with 4 is %.2f times more likely to be an alcohol abuser' %dodds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "\n",
    "The best way to compare accuracy of models and see how well your model generalize is to train (or fit) your model on a training dataset then test it on a separate test dataset. If you train and test your model on the same dataset (as we did above) you are likely to get a positvely biased result. The true generalizeability can only be observed when you test your model on a separate hold-out test dataset on which the model was not trained. A simple way to do cross-validation is to split the dataset before you begin your analysis. \n",
    "\n",
    "There are many ways to split the dataset but for simplicity we will use the train_test_split function in the module sci-kit learn.  \n",
    "We will split the data so that the training dataset consists 60% of the data and the test dataset consists 40% of the data.\n",
    "\n",
    "### 1. Split the data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:39:55.750103",
     "start_time": "2017-01-27T10:39:55.687554"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "np.random.seed(12345678) # set the random number generator seed to get consistent results\n",
    "trainratio = .6\n",
    "train, test = train_test_split(df, train_size=trainratio, random_state=1)\n",
    "print('train-test ratio %.2f' %(float(len(train))/(len(train)+len(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fit the model on the training dataset\n",
    "Now we fit a new model and save it to logmt.  \n",
    "We will try to predict 'Alcabuse' with 'freetime' and 'goout'  \n",
    "Note that we specify what dataset we are using by setting data = train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:42:00.202890",
     "start_time": "2017-01-27T10:42:00.184033"
    }
   },
   "outputs": [],
   "source": [
    "# We run the model on the train dataset\n",
    "logmt = smf.logit('Alcabuse ~ freetime + goout',data=train).fit()\n",
    "print(logmt.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate predictions on the test dataset\n",
    "We now use the model with coefficients estimated from the training dataset (logmt) and plug in the predictor variable values from the test dataset by using the .predict method. This method outputs the probabilities for us so we don't have to apply separate sigmoid transform. Therefore the output are the probabilities predicted with our model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:42:10.419775",
     "start_time": "2017-01-27T10:42:10.410035"
    }
   },
   "outputs": [],
   "source": [
    "ytestpred = logmt.predict(test)\n",
    "np.round(ytestpred,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate accuracy of the predictions\n",
    "To calculate the accuracy of the model we need to specify the cutoff for the probabilities to say whether one should be classified as an alcohol abuser or not. For simplicity, let's say that we'll classify the person as an alcohol abuser if the estimated probability is greater than 50%.  \n",
    "\n",
    "We will then calculate the accuracy of the model in classifying alcohol abuse for the training data and the test data separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T10:42:29.512250",
     "start_time": "2017-01-27T10:42:29.479882"
    }
   },
   "outputs": [],
   "source": [
    "cutoff = .5\n",
    "\n",
    "# Calculate accuracy for train\n",
    "ytrainpred = logmt.predict(train)\n",
    "ytrainpred[ytrainpred>=cutoff] =1 \n",
    "ytrainpred[ytrainpred<cutoff] =0 \n",
    "accs_train = pd.crosstab(ytrainpred,train[\"Alcabuse\"])\n",
    "\n",
    "# Calculate accuracy for test\n",
    "ytestpred = logmt.predict(test)\n",
    "ytestpred[ytestpred>=cutoff] =1 \n",
    "ytestpred[ytestpred<cutoff] =0 \n",
    "accs_test = pd.crosstab(ytestpred,test[\"Alcabuse\"])\n",
    "\n",
    "# Overall accuracy\n",
    "print('Overall Accuracy in Training set : %.2f ' %((accs_train[0][0]+accs_train[1][1])/float(train.shape[0])))\n",
    "print('overall Accuracy in Testing set  : %.2f ' %((accs_test[0][0]+accs_test[1][1])/float(test.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why do you think the accuracy is lower for the test data compared to the training?\n",
    "\n",
    "Accuracy is one number to represent the overall model performance, but doesn't differentiate between different types of classification errors such as false positives and false negatives.  Sensitivity and Specificity are useful metrics to assess these aspects of the model's performance.\n",
    "\n",
    " | **Predicted Positive** | **Predicted Negative**\n",
    "--- | --- | ---\n",
    "**Actual Positive** | 192 | 21\n",
    "**Actual Negative** | 8 | 16\n",
    "\n",
    "*Sensitivity* $ \\frac{TP}{TP+FN} $ : measures the proportion of positives that are correctly identified (e.g. we correctly identify the alcohol abusers)   \n",
    "*Specificity* $ \\frac{TN}{TN+FP} $ : measures the proportion of negatives that are correctly identified (e.g. we correctly identify the non-alcohol abusers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:02:18.086823",
     "start_time": "2017-01-27T11:02:18.060057"
    }
   },
   "outputs": [],
   "source": [
    "# Sensitivity and false negatives\n",
    "print('Training')\n",
    "display(accs_train)\n",
    "print('')\n",
    "print('sensitivity : %.2f ' %(float(accs_train[1][1])/(accs_train[1][1]+accs_train[1][0])))\n",
    "print('false negative : %.2f ' %(float(accs_train[1][0])/(accs_train[1][1]+accs_train[1][0])))\n",
    "print('specificity : %.2f ' %(float(accs_train[0][0])/(accs_train[0][0]+accs_train[0][1])))\n",
    "print('false positives : %.2f ' %(float(accs_train[0][1])/(accs_train[0][0]+accs_train[0][1])))\n",
    "\n",
    "# Specificity and false positives\n",
    "print('\\n')\n",
    "print('Test')\n",
    "display(accs_test)\n",
    "print('\\n')\n",
    "print('sensitivity : %.2f ' %(float(accs_test[1][1])/(accs_test[1][1]+accs_test[1][0])))\n",
    "print('false negative : %.2f ' %(float(accs_test[1][0])/(accs_test[1][1]+accs_test[1][0])))\n",
    "print('specificity : %.2f ' %(float(accs_test[0][0])/(accs_test[0][0]+accs_test[0][1])))\n",
    "print('false positives : %.2f ' %(float(accs_test[0][1])/(accs_test[0][0]+accs_test[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, sensitivity, and specificity are all affected if we change the cutoff value to .7\n",
    "So now, we classify the student as an alcohol abuser if there is more than 80% probability.     \n",
    "We can see that our overall accuracy increases from 84% to 85%.  \n",
    "We have a tradeoff of decreasing sensitivity to 28% but increasing specificity to 98%.   \n",
    "We are less likely to falsely accuse a student(decreased false positive), but we are less able to detect an abusing student (increased false negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:03:51.732735",
     "start_time": "2017-01-27T11:03:51.708175"
    }
   },
   "outputs": [],
   "source": [
    "ytestpred = logmt.predict(test)\n",
    "cutoff = .7\n",
    "ytestpred[ytestpred>=cutoff] =1 \n",
    "ytestpred[ytestpred<cutoff] =0 \n",
    "accs = pd.crosstab(ytestpred,test[\"Alcabuse\"])\n",
    "\n",
    "# Overall accuracy\n",
    "print ('overall accuracy : %.2f ' %((accs[0][0]+accs[1][1])/float(test.shape[0])))\n",
    "# Sensitivity and false negatives\n",
    "print('')\n",
    "print('sensitivity : %.2f ' %(float(accs[1][1])/(accs[1][1]+accs[1][0])))\n",
    "print('false negative : %.2f ' %(float(accs[1][0])/(accs[1][1]+accs[1][0])))\n",
    "\n",
    "# Specificity and false positives\n",
    "print('')\n",
    "print('specificity : %.2f ' %(float(accs[0][0])/(accs[0][0]+accs[0][1])))\n",
    "print('false positives : %.2f ' %(float(accs[0][1])/(accs[0][0]+accs[0][1])))\n",
    "\n",
    "display(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is a method to help deal with [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) and also avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting). Overfitting occurs when you have an overly complex model such as having more features(Xs) than observations(Y). For instance, if you try to fit 10 observations with 10 features, each coefficient can be adjusted for a perfect fit but it wouldn't generalize well. In other cases, you might face the problem of feature selection. If you have numerous variables, it is time consuming to try every single combination of features in your model to see what yields the best result. \n",
    "<img src=\"http://i.stack.imgur.com/0NbOY.png\">\n",
    "\n",
    "Regularization attempts to solve this problem by introducting a loss function that penalizes the model for each additional features added to the model. The AIC / BIC values we mentioned in the first session is a form of regularization in comparing models. \n",
    "\n",
    "In this section we will focus on feature selection regularization methods Lasso and Ridge regressions.\n",
    "\n",
    "## Lasso regression\n",
    "In short, [Lasso](http://stats.stackexchange.com/questions/17251/what-is-the-lasso-in-regression-analysis) is a feature selection method that reduces the number of features to use in a regression.  \n",
    "This is useful if you have a lot of variables that are correlated or you have more variables than observations.  \n",
    "[Here](http://studentlife.cs.dartmouth.edu/smartgpa.pdf) is a study from Dartmouth that used Lasso on the Student Life Dataset.\n",
    "\n",
    "In our example, we will try to find which variables can predict the level of alcohol consumption 'AlcSum'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:05:03.397385",
     "start_time": "2017-01-27T11:05:03.393882"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['AlcSum']=df['Dalc']+df['Walc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:05:06.581723",
     "start_time": "2017-01-27T11:05:06.577263"
    }
   },
   "outputs": [],
   "source": [
    "# We have a lot of features to choose from\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:05:19.241902",
     "start_time": "2017-01-27T11:05:18.929428"
    }
   },
   "outputs": [],
   "source": [
    "# Make a model with all the feature variables\n",
    "formula = 'AlcSum ~ school+sex+age+address+famsize+Pstatus+Medu+\\\n",
    "       Fedu+Mjob+Fjob+reason+guardian+traveltime+\\\n",
    "       studytime+failures+schoolsup+famsup+paid+activities+nursery+higher+\\\n",
    "       internet+romantic+famrel+freetime+goout+health+absences+G1+G2+G3'\n",
    "\n",
    "# Generate a design matrix with the formula\n",
    "# This automatically generates separate regressors for categorical variables.\n",
    "from patsy import dmatrices\n",
    "y, X = dmatrices(formula_like=formula, data=df, return_type='dataframe')\n",
    "# Ridge if L1_wt = 0 , Lasso if L1_wt = 1\n",
    "penalty = .036\n",
    "regm = smf.OLS(y,X).fit_regularized(method='coord_descent', maxiter=1000, alpha=penalty, L1_wt=1.0)\n",
    "print regm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we determine the penalty value? \n",
    "Essentially, we want to select the regularization parameter by identifying the one from a set of possible values (e.g. grid search) that results in the best fit of the model to the data.  However, it is important to note that it is easy to introduce bias into this process by trying a bunch of alphas and selecting the one that works best.  This can lead to optimistic evaluations of how well your model works.\n",
    "\n",
    "Cross-validation is an ideal method to deal with this.  We can use cross-validation to select the alpha while adding minimal bias to the overall model prediction.\n",
    "\n",
    "There are several different metrics of model fit to use as a criterion variable.  Log-likelihood or mean squared error are the two most common depending on how you are estimating the parameters (i.e., maximizing log-likelihood or minimizizing squared error).  The Akaike information criterion (AIC) and the Bayes Information criterion (BIC) are two common metrics that apply different penalties to these metrices for the number of free parameters in your model.\n",
    "\n",
    "Here we will demonstrate using both to select an optimal value of the regularization parameter alpha of the Lasso estimator from an example provided by [scikit-learn](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html). For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:11:14.478267",
     "start_time": "2017-01-27T11:11:14.157680"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "import time\n",
    "\n",
    "# LassoLarsIC: least angle regression with BIC/AIC criterion\n",
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(X, y)\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(X, y)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "def plot_ic_criterion(model, name, value, color):\n",
    "    alpha_ = model.alpha_\n",
    "    alphas_ = model.alphas_\n",
    "    criterion_ = model.criterion_\n",
    "    plt.plot(-np.log10(alphas_), criterion_, '--', color=color,\n",
    "             linewidth=3, label='%s criterion' % name)\n",
    "    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,\n",
    "                label='%s estimate alpha: %.3f' %(name,value) )\n",
    "    plt.xlabel('-log(alpha)',fontsize=16)\n",
    "    plt.ylabel('Model Fit Criterion',fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plot_ic_criterion(model_aic, 'AIC', alpha_aic_ , 'b')\n",
    "plot_ic_criterion(model_bic, 'BIC', alpha_bic_ , 'r')\n",
    "plt.legend()\n",
    "plt.title('Information-criterion for model selection',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a plot showing how well each alpha value performs across cross-validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:23:24.140077",
     "start_time": "2017-01-27T11:23:23.747748"
    }
   },
   "outputs": [],
   "source": [
    "# LassoCV: coordinate descent\n",
    "y, X = dmatrices(formula_like=formula, data=df, return_type='dataframe')\n",
    "\n",
    "# Compute paths\n",
    "print(\"Computing regularization path using the coordinate descent lasso...\")\n",
    "t1 = time.time()\n",
    "model = LassoCV(cv=5).fit(X, y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='CV estimate alpha: %.3f' %model.alpha_ )\n",
    "plt.legend()\n",
    "plt.xlabel('-log(alpha)',fontsize=16)\n",
    "plt.ylabel('Mean square error',fontsize=16)\n",
    "plt.title('Mean square error on each fold: coordinate descent',fontsize=18)\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "The goal of the ridge function is to choose a penalty λ for which the coefficients are not rapidly changing and have “sensible” signs. It is especially useful when data suffers from multicollinearity, that is some of your predictor variables are highly correlated. Unlike LASSO, ridge does not produce a sparse solution, but rather shrinks variables that are highly collinear towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:26:15.846904",
     "start_time": "2017-01-27T11:26:14.695671"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "y, X = dmatrices(formula_like=formula, data=df, return_type='dataframe')\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(-2, 6, n_alphas)\n",
    "\n",
    "# Get optimal lambda\n",
    "clf = linear_model.RidgeCV(alphas=alphas)\n",
    "clf.fit(X,y)\n",
    "aestim = clf.alpha_ # Estimated regularization param\n",
    "\n",
    "# Get paths\n",
    "clf = linear_model.Ridge(fit_intercept=False)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    clf.set_params(alpha=a)\n",
    "    clf.fit(X, y)\n",
    "    coefs.append(clf.coef_)\n",
    "\n",
    "###############################################################################\n",
    "# Display results\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, np.squeeze(coefs))\n",
    "ax.set_xscale('log')\n",
    "plt.axvline(aestim, linestyle='--', color='k', label='alpha: CV estimate')\n",
    "plt.legend()\n",
    "plt.xlabel('Lambda',fontsize=16)\n",
    "plt.ylabel('Weights',fontsize=16)\n",
    "plt.title('Ridge coefficients as a function of the regularization',fontsize=18)\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points on the left vertical axis (the left ends of the lines) are the ordinary least squares regression values.\n",
    "These occur for k equal zero. As k is increased, the values of the regression estimates change, often wildly at first.\n",
    "At some point, the coefficients seem to settle down and then gradually drift towards zero.\n",
    "\n",
    "The task of the ridge regression analyst is to determine at what value of k these coefficients are at their stable\n",
    "values. A vertical line is drawn at the value selected for reporting purposes. It is anticipated that you would run\n",
    "the program several times until an appropriate value of k is determined. In our case, the selected alpha value is indicated by a vertical line at which a = 65.93. The smaller the value of k, the smaller the amount of bias that is included in the estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:26:48.340880",
     "start_time": "2017-01-27T11:26:48.333706"
    }
   },
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(fit_intercept=False)\n",
    "a = aestim\n",
    "clf.set_params(alpha=a)\n",
    "clf.fit(X, y)\n",
    "np.round(clf.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-27T11:26:56.928052",
     "start_time": "2017-01-27T11:26:56.884641"
    }
   },
   "outputs": [],
   "source": [
    "# Ridge if L1_wt = 0 , Lasso if L1_wt = 1\n",
    "penalty = aestim\n",
    "regm = smf.OLS(y,X).fit_regularized(method='coord_descent', maxiter=1000, alpha=penalty, L1_wt=0)\n",
    "print regm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Individual exercise\n",
    "Below is the information about the dataset. \n",
    "\n",
    "Attribute Information:  \n",
    "1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)   \n",
    "2 sex - student's sex (binary: 'F' - female or 'M' - male)   \n",
    "3 age - student's age (numeric: from 15 to 22)   \n",
    "4 address - student's home address type (binary: 'U' - urban or 'R' - rural)   \n",
    "5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)   \n",
    "6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)   \n",
    "7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 Ã¢â‚¬â€œ 5th to 9th grade, 3 Ã¢â‚¬â€œ secondary education or 4 Ã¢â‚¬â€œ higher education)   \n",
    "8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 Ã¢â‚¬â€œ 5th to 9th grade, 3 Ã¢â‚¬â€œ secondary education or 4 Ã¢â‚¬â€œ higher education)   \n",
    "9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')   \n",
    "10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')   \n",
    "11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')   \n",
    "12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')   \n",
    "13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)   \n",
    "14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)   \n",
    "15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)   \n",
    "16 schoolsup - extra educational support (binary: yes or no)   \n",
    "17 famsup - family educational support (binary: yes or no)   \n",
    "18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)   \n",
    "19 activities - extra-curricular activities (binary: yes or no)   \n",
    "20 nursery - attended nursery school (binary: yes or no)   \n",
    "21 higher - wants to take higher education (binary: yes or no)   \n",
    "22 internet - Internet access at home (binary: yes or no)   \n",
    "23 romantic - with a romantic relationship (binary: yes or no)   \n",
    "24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)   \n",
    "25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)   \n",
    "26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)   \n",
    "27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)   \n",
    "28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)   \n",
    "29 health - current health status (numeric: from 1 - very bad to 5 - very good)   \n",
    "30 absences - number of school absences (numeric: from 0 to 93)     \n",
    "31 G1 - first period grade (numeric: from 0 to 20)   \n",
    "31 G2 - second period grade (numeric: from 0 to 20)   \n",
    "32 G3 - final grade (numeric: from 0 to 20, output target)   \n",
    "\n",
    "## Exercise 1\n",
    "Before you begin, you may want to replace all the 'yes' or 'no' responses in the dataframe to numerical values such as 1 or 0\n",
    "using the df.replace() method. \n",
    "\n",
    "Run a logistic model to see whether studytime predicts 'wanting to take higher education'. \n",
    "\n",
    "Based on the model, what is the probability that John wants to pursue higher education if his\n",
    "studytime value is 2 (he studies 2 hours/week)?\n",
    "\n",
    "Based on the model what is the probability that Laura wants to pursue higher education if her\n",
    "studytime value is 3 (she studies 5 hours per week)? \n",
    "\n",
    "How much more likely (in odds) is Laura likely to want to pursue higher education than John ?\n",
    "\n",
    "## Exercise 2\n",
    "Run a LASSO regression to find what variables predict studytime.   \n",
    "Use the penalty parameter (alpha) estimated through cross validation of coordinate descent.  \n",
    "What is your interpretation of the results?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
